import requests
from bs4 import BeautifulSoup
import pandas as pd

print("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")

# 1. ã‚µã‚¤ãƒˆæƒ…å ±ï¼ˆDay 20ã¨åŒã˜ Pythonå…¬å¼ã‚µã‚¤ãƒˆï¼‰
url = "https://www.python.org/"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, "html.parser")

# 2. ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã‚‹ã€Œç©ºã®ãƒªã‚¹ãƒˆã€ã‚’æº–å‚™
data_list = []

# 3. ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆDay 20ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰
news_widget = soup.find("div", class_="blog-widget")

if news_widget:
    news_items = news_widget.find_all("li")
    
    for item in news_items:
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã‚’å–å¾—
        title = item.find("a").text
        link = item.find("a")["href"]
        
        # URLãŒ "/downloads/..." ã®ã‚ˆã†ã«çœç•¥ã•ã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ã®ã§è£œå®Œã™ã‚‹
        full_url = f"https://www.python.org{link}" 
        
        # è¾æ›¸ï¼ˆDictionaryï¼‰ã«ã¾ã¨ã‚ã‚‹
        data = {
            "Title": title,
            "URL": full_url
        }
        
        # ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆappendï¼‰
        data_list.append(data)

# 4. è¡¨ï¼ˆDataFrameï¼‰ã«å¤‰æ›
df = pd.DataFrame(data_list)

# çµæœã‚’è¡¨ç¤º
print("\nğŸ“Š å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿:")
print(df)

# 5. CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
# index=False: è¡Œç•ªå·(0,1,2...)ã‚’ä¿å­˜ã—ãªã„è¨­å®š
# encoding="utf-8_sig": Excelã§é–‹ã„ãŸã¨ãã®æ–‡å­—åŒ–ã‘ã‚’é˜²ããŠã¾ã˜ãªã„
df.to_csv("python_news.csv", index=False, encoding="utf-8_sig")

print("\nğŸ’¾ 'python_news.csv' ã«ä¿å­˜ã—ã¾ã—ãŸï¼")