import requests
from bs4 import BeautifulSoup
import pandas as pd
import time # é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã§ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ãŸã‚ã®ã€Œå¾…æ©Ÿã€ç”¨

# --- éƒ¨å“ï¼ˆé–¢æ•°ï¼‰ã‚¨ãƒªã‚¢ ---

def get_soup(target_url):
    """
    HTMLã‚’å–å¾—ã—ã€BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã‚¬ãƒ¼ãƒ‰ä»˜ãï¼‰
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124"
    }
    
    # ã€é‡è¦ã€‘ try-exceptæ§‹æ–‡
    # ã€Œtryã€ã®ä¸­ã‚’å®Ÿè¡Œã—ã¦ã¿ã¦ã€ã‚‚ã—ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã‚‰ã€Œexceptã€ã«é€ƒã’ã‚‹
    try:
        response = requests.get(target_url, headers=headers, timeout=10) # 10ç§’å¾…ã£ã¦ã‚‚ãƒ€ãƒ¡ãªã‚‰è«¦ã‚ã‚‹
        response.raise_for_status() # 404ã‚¨ãƒ©ãƒ¼ãªã©ãŒã‚ã‚Œã°ã“ã“ã§æ¤œçŸ¥
        soup = BeautifulSoup(response.text, "html.parser")
        return soup
    
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã‚‰ã€æ­¢ã¾ã‚‰ãšã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã ã‘å‡ºã—ã¦ None (ç©ºã£ã½) ã‚’è¿”ã™
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {target_url}")
        print(f"   ç†ç”±: {e}")
        return None

def extract_python_news(soup):
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    """
    # soupãŒç©ºã£ã½ï¼ˆã‚¨ãƒ©ãƒ¼ã ã£ãŸï¼‰å ´åˆã¯ã€ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¦çµ‚ã‚ã‚‹
    if soup is None:
        return []

    data_list = []
    # â€»ãƒšãƒ¼ã‚¸ã«ã‚ˆã£ã¦æ§‹é€ ãŒé•ã†å ´åˆãŒã‚ã‚‹ãŸã‚ã€æ±ç”¨çš„ã«å‹•ãã‹ç¢ºèªãŒå¿…è¦ã§ã™ãŒ
    # ä»Šå›ã¯Pythonå…¬å¼ã‚µã‚¤ãƒˆå†…ã®åŒã˜æ§‹é€ ã®ãƒšãƒ¼ã‚¸ã‚’æƒ³å®šã—ã¾ã™
    news_widget = soup.find("div", class_="blog-widget")

    if news_widget:
        news_items = news_widget.find_all("li")
        for item in news_items:
            title = item.find("a").text
            link = item.find("a")["href"]
            
            # URLãŒ http ã‹ã‚‰å§‹ã¾ã£ã¦ã„ãªã„å ´åˆã¯è£œå®Œã™ã‚‹
            if not link.startswith("http"):
                link = f"https://www.python.org{link}"
            
            data = {
                "Title": title,
                "URL": link
            }
            data_list.append(data)
            
    return data_list

def save_to_csv(data_list, filename):
    if not data_list:
        print("âš ï¸ ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    df = pd.DataFrame(data_list)
    df.to_csv(filename, index=False, encoding="utf-8_sig")
    print(f"ğŸ’¾ '{filename}' ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼(ä»¶æ•°: {len(df)}ä»¶)")

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒªã‚¢ ---

def main():
    print("ğŸš€ ã‚¿ãƒ•ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒœãƒƒãƒˆèµ·å‹•")
    
    # 1. å·¡å›ã—ãŸã„URLã®ãƒªã‚¹ãƒˆ
    url_list = [
        "https://www.python.org/",          # å­˜åœ¨ã™ã‚‹ãƒšãƒ¼ã‚¸
        "https://www.python.org/invalid",   # å­˜åœ¨ã—ãªã„ãƒšãƒ¼ã‚¸ï¼ˆã‚ã–ã¨ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹å®Ÿé¨“ç”¨ï¼‰
        "https://www.python.org/psf/"       # å­˜åœ¨ã™ã‚‹ãƒšãƒ¼ã‚¸ï¼ˆPSFæƒ…å ±ï¼‰
    ]
    
    all_data = [] # å…¨ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã“ã“ã«é›†ã‚ã‚‹

    # 2. ãƒªã‚¹ãƒˆã®ä¸­èº«ã‚’é †ç•ªã«å‡¦ç†
    for url in url_list:
        print(f"\nğŸ“¡ {url} ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        
        # å–å¾—
        soup = get_soup(url)
        
        # æŠ½å‡º
        news = extract_python_news(soup)
        
        # è¦‹ã¤ã‹ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚’å…¨ä½“ã®ç®±ã«è¿½åŠ ï¼ˆextendã¯ãƒªã‚¹ãƒˆåŒå£«ã‚’çµåˆã™ã‚‹å‘½ä»¤ï¼‰
        if news:
            all_data.extend(news)
            print(f"   âœ… {len(news)} ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—")
        else:
            print("   ğŸ’¨ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        # ã‚µãƒ¼ãƒãƒ¼ã¸ã®ç¤¼å„€ã¨ã—ã¦1ç§’ä¼‘ã‚€
        time.sleep(1)

    # 3. æœ€å¾Œã«ã¾ã¨ã‚ã¦ä¿å­˜
    save_to_csv(all_data, "python_news_multi.csv")
    print("\nâœ… å…¨å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()